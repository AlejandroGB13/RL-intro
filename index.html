<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="keywords" content="remark,remarkjs,markdown,slideshow,presentation" />
    <meta name="description" content="A simple, in-browser, markdown-driven slideshow tool." />
    <title>RL introduction</title>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Droid Serif';
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">
name: inverse
layout: true
class: center, middle, inverse
---

# Introducción a Reinforcement Learning (RL) en procesos de control
Alejandro González Barberá 

.footnote[Universidad Jaume I (UJI), Grupo Fluidos Multifásicos (GFM)]
      
---
## ¿Qué es Reinforcement Learning?
---
layout: false
.left-column[
  ## Introducción
]
.right-column[

- Aprendizaje por refuerzo (RL) es un tipo de aprendizaje automático donde un agente interactúa con un entorno, observando estados y recibiendo recompensas.

- El objetivo del agente es aprender una política que maximice las recompensas a largo plazo.

- En el contexto de Process Control, el entorno podría ser una planta de procesos y las recompensas estarían relacionadas con la eficiencia, la seguridad y la calidad del producto.
      
]
---
.left-column[
  ## Introducción
  ## ¿Cómo funciona el aprendizaje por refuerzo?
]
.right-column[

- El agente interactúa con el entorno tomando acciones y observando los estados y las recompensas resultantes.

- Con el tiempo, el agente aprende una política que asigna acciones a estados de manera que maximiza la recompensa a largo plazo.

- El aprendizaje por refuerzo se basa en la idea de que un agente puede aprender de sus propias experiencias, sin necesidad de que se le proporcione un conjunto de datos previamente etiquetados.

]   
---
.left-column[
  ## Introducción
  ## ¿Cómo funciona el aprendizaje por refuerzo?
  ## Tipos de Agente
]
.right-column[

- Q-Learning: Q-Learning es un algoritmo libre de modelo y off-policy que aprende la función Q-valor óptima para el MDP. Espacios de acción discretos.

- SARSA: SARSA es otro algoritmo libre de modelo y on-policy que aprende la función Q-valor. Espacios de acción continuos.

- Redes Neuronales Q Profundas (DQN): DQN es un algoritmo de aprendizaje por refuerzo profundo que combina Q-Learning con redes neuronales profundas para aprender una función Q-valor. Espacios de estado de alta dimensionalidad.
  
- Métodos de Gradiente de Política: Algoritmos libres de modelo que aprenden directamente la política mediante la optimización de un objetivo de rendimiento. Espacios de acción continuos y problemas en los que la exploración es difícil.
  
- Métodos Actor-Crítico: Los métodos actor-crítico combinan las ventajas de los métodos basados en el valor y los basados en la política al aprender simultáneamente tanto una función de valor como una política. Espacios de acción continuos.      

]   
      
---
.left-column[
  ## Introducción
  ## ¿Cómo funciona el aprendizaje por refuerzo?
  ## Tipos de Agente
  ## Ejemplos
]
.right-column[

- El aprendizaje por refuerzo se ha aplicado con éxito a una amplia variedad de problemas de Process Control.

- Ejemplos incluyen control de temperatura en hornos, control de nivel en tanques de almacenamiento, y control de flujo en tuberías.

- El RL se utiliza para aprender una política que ajusta automáticamente los parámetros de control en tiempo real, lo que puede llevar a mejoras significativas en la eficiencia y la calidad del producto.

]   
      
---
template: inverse

## Workflow
---
      layout: false
.left-column[
  ## Pasos a seguir
]
.right-column[

- Definir el problema: Identificar el problema que se quiere resolver y definir el ambiente de simulación.

- Definir el espacio de estado y de acción: Seleccionar las variables que se utilizarán como estado y las acciones posibles que se pueden tomar.
      
- Definir la función de recompensa: Establecer una función que proporcione una recompensa para cada acción tomada.
    
- Seleccionar un algoritmo de Reinforcement.

- Entrenar el agente.
      
- Evaluar y ajustar el agente: Probar la política del agente en un ambiente distinto y ajustarla según sea necesario.
      
- Desplegar el agente: Implementar la política aprendida por el agente en el ambiente real y supervisar su desempeño.
      
]
      
---
template: inverse

## Toy problem
---
layout: false
.left-column[
  ## Introducción
]
.right-column[

Reactores con depósito agitado continuo (CSTR):
      
- El problema del reactor consiste en controlar la temperatura y la concentración de un reactor químico mediante la manipulación de las entradas del sistema.

- La dinámica del reactor es altamente no lineal y afectada por múltiples factores, incluyendo la velocidad de flujo, la temperatura de entrada, la concentración de alimentación, entre otros.
  
- El enfoque de reinforcement learning busca construir un agente que aprenda por sí mismo a controlar el proceso, sin requerir un modelo preciso del sistema ni un ajuste manual de los parámetros.
      
]
---
.left-column[
  ## Introducción
  ## Deeper look
]
.right-column[

Inputs:

    V: reactor volume (m^3)
    q: volumetric flow rate (m^3/s)
    c0: initial concentrations of the reactants [Ca0, Cb0, Cc0] (mol/m^3)
    k0: pre-exponential factor in the Arrhenius equation (1/s)
    Ea: activation energy (J/mol)
    R: gas constant (J/(mol*K))
    deltaH: heat of reaction (J/mol)

Outputs:

    y: concentration of the product Cc at the outlet (mol/m^3)
      
]   
      
---
.left-column[
  ## Introducción
  ## Deeper look
]
.right-column[

- Se busca maximizar la concentración de producto Cc en la salida del reactor (mol/m^3).

- Minimizar la entrada de flujo.

- Se utiliza un agente de reinforcement learning para optimizar la salida del reactor mediante la modificación de la tasa de flujo de entrada.
  
- La optimización se realiza mediante la maximización de una función de recompensa que penaliza el uso excesivo de la tasa de flujo y la distancia de la concentración del producto Cc en la salida del reactor de su valor objetivo.
      
]   
---
.left-column[
  ## Introducción
  ## Deeper look
  ## Pasos
]
.right-column[

Pasos seguidos en líneas generales:

- Se define la clase CSTR que contiene los parámetros y las ecuaciones necesarias para simular el reactor.

- Se define la función de recompensa compute_reward que se utilizará para evaluar el desempeño del agente.

- Se define la función reset que se utiliza para reiniciar la simulación del reactor.

- Se define la función step que se utiliza para avanzar un paso en la simulación del reactor.

- Se crea una instancia de la clase CSTR y se inicializan los parámetros.

- Se crea una instancia del entorno gym.Env utilizando la clase CSTR.

- Se define la estructura de la red neuronal que se utilizará para aproximar la política del agente.

- Se entrena el agente utilizando el algoritmo proximal policy optimization (PPO) para que aprenda a optimizar la producción del reactor.

- Se realiza una simulación final utilizando el agente entrenado para evaluar su desempeño en el control del reactor.      

]   
      
---
name: last-page
template: inverse

## FIN

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create();
    </script>
  </body>
</html>
